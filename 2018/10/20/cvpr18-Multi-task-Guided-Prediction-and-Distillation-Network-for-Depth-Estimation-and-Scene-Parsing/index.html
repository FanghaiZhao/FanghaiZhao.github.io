<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Fanghai's blog"><meta name="keywords" content="PhD, Technique, Life"><title>[cvpr18] Multi-task Guided Prediction and Distillation Network for Depth Estimation and Scene Parsing | Update</title><link rel="stylesheet" type="text/css" href="//fonts.neworld.org/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">[cvpr18] Multi-task Guided Prediction and Distillation Network for Depth Estimation and Scene Parsing</h1><a id="logo" href="/.">Update</a><p class="description">Keep updating...</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">[cvpr18] Multi-task Guided Prediction and Distillation Network for Depth Estimation and Scene Parsing</h1><div class="post-meta"><a href="/2018/10/20/cvpr18-Multi-task-Guided-Prediction-and-Distillation-Network-for-Depth-Estimation-and-Scene-Parsing/#comments" class="comment-count"></a><p><span class="date">Oct 20, 2018</span><span><a href="/categories/Paper-Notes/" class="category">Paper Notes</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>This paper tackles the problem of simultaneous depth prediction and scene parsing. More specifically, using the same input data to predict the two tasks. </p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.Problems of traditionally direct multi-task learning:</p>
<ul>
<li>Mainly focus on the final prediction stage with cross-modal interactions or other refinement methods.</li>
<li>Distinct objectives for different tasks make it complicated for the model to be well optimized for both two tasks. In this way, the multi-task performance on some tasks may be even worse than optimizing the single task. </li>
</ul>
<p>2.Benefits of the newly proposed method</p>
<p>This paper proposes to learn intermediate auxiliary tasks as multi-modal inputs. </p>
<ul>
<li>It is well known that multi-modal data improves the performance of deep predictions. </li>
<li>The network could also learn to predict other related information as multi-modal inputs, such as contours and surface normals.</li>
<li>How to effectively exploit the multi-modal data to benefit the final predictions is crucially important. Current deep multi-task learning models assume only the single-modal data. </li>
</ul>
<p>3.Graphical illustration<br><img src="/images/20181019_paperNotes_1_2.jpg" alt="motivation"></p>
<h2 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h2><p><img src="/images/20181019_paperNotes_1_1.jpg" alt="framework"></p>
<h2 id="Techniques"><a href="#Techniques" class="headerlink" title="Techniques"></a>Techniques</h2><p>1.Multi-task learning</p>
<p>In contrast to the traditional multi-task learning, this paper proposes a new means, which predicts intermediate auxiliary tasks and utilize the multi-task predictions to refine the final predictions. This will help the model learn better feature representations.</p>
<p>2.Multi-modal distillation</p>
<p>Three multi-modal distillation modules are proposed to incorporate information from different predictions. It seems the multi-modal distillation involves in multiple source information fusion and how to maximally exploit information from various sources to improve the final result. </p>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>1.Network Optimization VS Network Training</p>
<p>Usually the topic of ‘network optimization’ follows after the main method and presents in the last paragraphs of the ‘Method’ section. Its content is usually the illustration of the loss functions to train the whole model. It seems that if the whole model is feed-forward, then you can say ‘end-to-end network optimization.’</p>
<p>The topic of ‘network training’ seems usually appear in the first paragraphs of the ‘Experiment’ section, and may be contained in the ‘Implementation details’ sub-section and is illustrated with the learning rate, training epochs, initialization, and other network parameters involved during training. While the ‘network optimization’ does not talk about the in-network parameters and just the supervision signals. When talking about ‘network training,’ you can say ‘stage-wise’ training when you want to train the first a few parts of the network and then the following so that the model could be better initialized and be better trained. </p>
<p>2.If you introduce a new module, it is important to verify the performance improvement is not caused due to the enlarged model capacity.</p>
<p>One solution for demonstration is that you can choose a different input to the module to compare with your preferred input to this module to verify the improvement is not due to the model capacity. </p>
</div><div class="tags"><a href="/tags/multitask/">multitask</a><a href="/tags/multi-modal-distillation/">multi-modal distillation</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80MDI2NC8xNjc5MQ=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Task"><span class="toc-text">Task</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Motivation"><span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Framework"><span class="toc-text">Framework</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Techniques"><span class="toc-text">Techniques</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Others"><span class="toc-text">Others</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/10/20/cvpr18-Multi-task-Guided-Prediction-and-Distillation-Network-for-Depth-Estimation-and-Scene-Parsing/">[cvpr18] Multi-task Guided Prediction and Distillation Network for Depth Estimation and Scene Parsing</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Notes/">Paper Notes</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/multitask/" style="font-size: 15px;">multitask</a> <a href="/tags/multi-modal-distillation/" style="font-size: 15px;">multi-modal distillation</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Fanghai.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script></body></html>